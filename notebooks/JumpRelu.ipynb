{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4b28a1-d0c3-42ac-bb9e-2ecf0487c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from overcomplete.sae import JumpSAE, train_sae\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1892fb-b431-451f-b7f0-0cec7e8e327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2293530/3713506127.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(train_path)\n"
     ]
    }
   ],
   "source": [
    "train_path = \"./logs/activation.pt\"\n",
    "ckpt = torch.load(train_path)\n",
    "\n",
    "fc1 = ckpt[\"fc1\"]\n",
    "fc1_activations = ckpt[\"fc1_activations\"]\n",
    "labels = ckpt[\"labels\"]\n",
    "\n",
    "batch_size = 1024\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(TensorDataset(fc1), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1dd6d7-1d51-4041-a2d9-d57e3a4ad7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/30], Loss: 747.2267, R2: 0.8285, L0: 92.1096, Dead Features: 7.8%, Time: 208.8014 seconds\n",
      "Epoch[2/30], Loss: 697.1829, R2: 0.9881, L0: 100.2846, Dead Features: 53.9%, Time: 166.4026 seconds\n",
      "Epoch[3/30], Loss: 650.5390, R2: 0.9936, L0: 103.5450, Dead Features: 53.9%, Time: 188.6200 seconds\n",
      "Epoch[4/30], Loss: 604.4399, R2: 0.9950, L0: 103.5547, Dead Features: 53.9%, Time: 189.4940 seconds\n",
      "Epoch[5/30], Loss: 558.5914, R2: 0.9957, L0: 103.4471, Dead Features: 53.9%, Time: 195.2696 seconds\n",
      "Epoch[6/30], Loss: 512.8776, R2: 0.9962, L0: 103.2384, Dead Features: 53.9%, Time: 172.2337 seconds\n",
      "Epoch[7/30], Loss: 467.2673, R2: 0.9965, L0: 102.8322, Dead Features: 53.9%, Time: 185.0890 seconds\n",
      "Epoch[8/30], Loss: 421.7168, R2: 0.9969, L0: 102.4130, Dead Features: 54.3%, Time: 211.0061 seconds\n",
      "Epoch[9/30], Loss: 376.1782, R2: 0.9971, L0: 101.7326, Dead Features: 54.3%, Time: 172.6667 seconds\n",
      "Epoch[10/30], Loss: 330.6875, R2: 0.9974, L0: 100.4855, Dead Features: 54.7%, Time: 167.2528 seconds\n",
      "Epoch[11/30], Loss: 285.2201, R2: 0.9973, L0: 98.5013, Dead Features: 55.5%, Time: 182.5191 seconds\n",
      "Epoch[12/30], Loss: 239.7434, R2: 0.9975, L0: 95.4730, Dead Features: 55.5%, Time: 186.4970 seconds\n",
      "Epoch[13/30], Loss: 194.2701, R2: 0.9973, L0: 90.3360, Dead Features: 55.5%, Time: 173.5659 seconds\n",
      "Epoch[14/30], Loss: 148.8132, R2: 0.9969, L0: 82.9119, Dead Features: 57.4%, Time: 191.4598 seconds\n",
      "Epoch[15/30], Loss: 103.4018, R2: 0.9963, L0: 74.3508, Dead Features: 59.0%, Time: 84.7489 seconds\n",
      "Epoch[16/30], Loss: 58.0772, R2: 0.9953, L0: 63.1662, Dead Features: 61.3%, Time: 33.5964 seconds\n",
      "Epoch[17/30], Loss: 12.8650, R2: 0.9943, L0: 50.3766, Dead Features: 64.8%, Time: 35.9728 seconds\n",
      "Epoch[18/30], Loss: -32.1575, R2: 0.9933, L0: 39.3963, Dead Features: 69.1%, Time: 38.8061 seconds\n",
      "Epoch[19/30], Loss: -76.8827, R2: 0.9924, L0: 31.0933, Dead Features: 73.4%, Time: 37.5049 seconds\n",
      "Epoch[20/30], Loss: -121.1440, R2: 0.9915, L0: 25.3883, Dead Features: 78.1%, Time: 40.6236 seconds\n",
      "Epoch[21/30], Loss: -164.7247, R2: 0.9909, L0: 19.4016, Dead Features: 81.2%, Time: 41.3190 seconds\n",
      "Epoch[22/30], Loss: -207.7933, R2: 0.9910, L0: 16.4768, Dead Features: 84.8%, Time: 38.1136 seconds\n",
      "Epoch[23/30], Loss: -250.4113, R2: 0.9903, L0: 14.6201, Dead Features: 87.9%, Time: 33.9660 seconds\n",
      "Epoch[24/30], Loss: -183.4845, R2: 0.9855, L0: 12.6189, Dead Features: 89.5%, Time: 35.1541 seconds\n",
      "Epoch[25/30], Loss: -20.1263, R2: 0.9812, L0: 11.9224, Dead Features: 90.2%, Time: 37.9325 seconds\n",
      "Epoch[26/30], Loss: -82.2559, R2: 0.9833, L0: 12.2211, Dead Features: 90.2%, Time: 36.4912 seconds\n",
      "Epoch[27/30], Loss: -134.1206, R2: 0.9866, L0: 12.7943, Dead Features: 91.4%, Time: 37.2741 seconds\n",
      "Epoch[28/30], Loss: -346.1730, R2: 0.9917, L0: 13.3370, Dead Features: 92.2%, Time: 38.4656 seconds\n",
      "Epoch[29/30], Loss: -392.9124, R2: 0.9925, L0: 13.4044, Dead Features: 92.6%, Time: 43.2008 seconds\n",
      "Epoch[30/30], Loss: -439.6711, R2: 0.9923, L0: 13.1464, Dead Features: 92.6%, Time: 39.1292 seconds\n"
     ]
    }
   ],
   "source": [
    "sae = JumpSAE(fc1.shape[-1], nb_concepts=256,\n",
    "              bandwith=1e-2, kernel='silverman', device='cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(sae.parameters(), lr=3e-3)\n",
    "\n",
    "desired_sparsity = 0.05\n",
    "\n",
    "def criterion(x, x_hat, pre_codes, codes, dictionary):\n",
    "  # here we directly use the thresholds of the model to control the sparsity\n",
    "  loss = (x - x_hat).square().mean()\n",
    "\n",
    "  sparsity = (codes > 0).float().mean().detach()\n",
    "  if sparsity > desired_sparsity:\n",
    "    # if we are not sparse enough, increase the thresholds levels\n",
    "    loss -= sae.thresholds.sum()\n",
    "\n",
    "  return loss\n",
    "\n",
    "logs = train_sae(sae, dataloader, criterion, optimizer, nb_epochs=30, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
